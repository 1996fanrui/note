# hive中常用的文件格式介绍

## textFile
- textFile指用普通的文本方式进行存储的文件

### DDL
```
create table test.table_name (
     id                 int
    ,uid                bigint
) partitioned by (ds string)
row format delimited
fields terminated by '\001'; 		-- 不可见字符作为字段分隔符
```

> 注：默认情况为text格式的文件


## Parquet
- parquet-format详细介绍移步至 [github](https://github.com/apache/parquet-format)
- 列式存储示意图

![列式存储示意图](https://img-blog.csdn.net/20160616112127928)


### DDL
```
create table test.table_name (
    stat_date     string
   ,module        string
) partitioned by (ds string) 
stored as parquet -- parquet格式存储
tblproperties("orc.compress"="SNAPPY"); -- snappy压缩
```
- show create table

```
CREATE TABLE `table_name`(
  `stat_date` string,                    
  `module` string) 
PARTITIONED BY (`ds` string)                           
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'  
STORED AS INPUTFORMAT  'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'  
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' 
LOCATION 'hdfs://nameservice/user/hive/warehouse/test.db/table_name' 
TBLPROPERTIES ('orc.compress'='SNAPPY', 'transient_lastDdlTime'='1517652215') 
;
```
> 这里默认使用的是`MapredParquetInputFormat` 和 `MapredParquetOutputFormat`进行输入输出格式化


### 名词介绍
  - Block(HDFS block): This means a block in HDFS and the meaning is
    unchanged for describing this file format.  The file format is
    designed to work well on top of HDFS.

  - File(HDFS file): A HDFS file that must include the metadata for the file.
    It does not need to actually contain the data.

  - Row group(行组): 将数据逻辑水平分区为行。没有为行组保证的物理结构。行组由数据集中每列的列块组成。

  - Column chunk(列块): 特定列的一大块数据。它们位于特定的行组中，并保证在文件中是连续的。

  - Page(页): 列块分为页面。页面在概念上是不可分割的单元（就压缩和编码而言）。可以有多个页面类型在列块中交错。

> 从层次结构来看，文件由一个或多个行组组成。行组每列只包含一个列块。列块包含一个或多个页面。  
> 按照行切分行组，每个行组的数据在磁盘中是连续的，每个行组内按照列的不同分为不同的列块，每个列对应一个列块，相同列块的数据在磁盘中是连续的，每个列块包含一个或多个页面。  
> > 列式存储，所以相同列的数据在内存中连续，这就意味着相同行的数据在内存中并不连续。


### File format
This file and the [thrift definition](src/main/thrift/parquet.thrift) should be read together to understand the format.

    4-byte magic number "PAR1"
    <Column 1 Chunk 1 + Column Metadata>
    <Column 2 Chunk 1 + Column Metadata>
    ...
    <Column N Chunk 1 + Column Metadata>
    <Column 1 Chunk 2 + Column Metadata>
    <Column 2 Chunk 2 + Column Metadata>
    ...
    <Column N Chunk 2 + Column Metadata>
    ...
    <Column 1 Chunk M + Column Metadata>
    <Column 2 Chunk M + Column Metadata>
    ...
    <Column N Chunk M + Column Metadata>
    File Metadata
    4-byte length in bytes of file metadata (little endian)
    4-byte magic number "PAR1"

In the above example, there are N columns in this table, split into M row
groups.  The file metadata contains the locations of all the column metadata
start locations.  More details on what is contained in the metadata can be found
in the thrift definition.

在数据之后写入元数据以允许单遍写入。

Readers应首先读取文件元数据以找到他们感兴趣的所有列块。然后应按顺序读取列块。

![File Layout](https://raw.github.com/apache/parquet-format/master/doc/images/FileLayout.gif)


### Metadata

There are three types of metadata: `file metadata`, `column (chunk) metadata` and `page
header metadata`.  All thrift structures are serialized using the TCompactProtocol.

 ![Metadata diagram](https://github.com/apache/parquet-format/raw/master/doc/images/FileFormat.gif)


### 嵌套数据模型

Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段有三个属性：重复次数、数据类型和字段名，重复次数可以是以下三种：required(只出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种：group(复杂类型)和primitive(基本类型)。例如Dremel中提供的Document的schema示例，它的定义如下：

```
message Document {
  required int64 DocId;
  optional group Links {
    repeated int64 Backward;
    repeated int64 Forward; 
  }
  repeated group Name {
    repeated group Language {
      required string Code;
      optional string Country; 
     }
    optional string Url; 
  }
}
```

可以把这个Schema转换成树状结构，根节点可以理解为repeated类型，如图

![](https://img-blog.csdn.net/20160616142924507)

可以看出在Schema中所有的基本类型字段都是叶子节点，在这个Schema中一共存在6个叶子节点，如果把这样的Schema转换成扁平式的关系模型，就可以理解为该表包含六个列。Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现的。由于一条记录中某一列可能出现零次或者多次，需要标示出哪些列的值构成一条完整的记录。这是由Striping/Assembly算法实现的。

由于Parquet支持的数据模型比较松散，可能一条记录中存在比较深的嵌套关系，如果为每一条记录都维护一个类似的树状结可能会占用较大的存储空间，因此Dremel论文中提出了一种高效的对于嵌套数据格式的压缩算法：Striping/Assembly算法。它的原理是每一个记录中的每一个成员值有三部分组成：Value、Repetition level和Definition level。value记录了该成员的原始值，可以根据特定类型的压缩算法进行压缩，两个level值用于记录该值在整个记录中的位置。对于repeated类型的列，Repetition level值记录了当前值属于哪一条记录以及它处于该记录的什么位置；对于repeated和optional类型的列，可能一条记录中某一列是没有值的，假设我们不记录这样的值就会导致本该属于下一条记录的值被当做当前记录的一部分，从而造成数据的错误，因此对于这种情况需要一个占位符标示这种情况。

通过Striping/Assembly算法，parquet可以使用较少的存储空间表示复杂的嵌套格式，并且通常Repetition level和Definition level都是较小的整数值，可以通过RLE算法对其进行压缩，进一步降低存储空间。

[The striping and assembly algorithms from the Dremel paper详解](https://github.com/julienledem/redelm/wiki/The-striping-and-assembly-algorithms-from-the-Dremel-paper)

[dremel论文](http://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36632.pdf)

> ### 用该算法为什么能优化嵌套的列式存储，还不清楚啊！！！
> 答：该存储算法，叶子节点都是基本类型，上述例子中，相当于有6列，具体的字段，可能对应着三种情况:required、optional、repeated，这里采用列式存储时，一行数据的某个字段可能不对应值，可能对应一个，可能对应多个，为了清晰的区分某行到底有没有值或者有几个值，所以采用两个标志 R、D，根据R可以判断该行是否结束，根据D可以判断该行是否有值

### 优势
1. parquet由于每一列的成员都是同构的，可以针对不同的数据类型使用更高效的数据压缩算法(相同的`row group`，不同的列可能采用不同的压缩算法)，进一步减小I/O。CSV格式一般不进行压缩，通过parquet存储数据有效的节约了空间，不考虑备份情况下，压缩比将近27倍（parquet有四种压缩方式lzo、gzip、snappy、uncompressed，其中默认gzip的压缩方式，其压缩率最高，压缩解压的速率最快）
2. 查询的时候不需要扫描全部的数据，而只需要读取每次查询涉及的列，这样可以将I/O消耗降低N倍，另外可以保存每一列的统计信息(min、max、sum等)，具体参考[thrift definition](src/main/thrift/parquet.thrift) 可以看到存储方式
3. 分区过滤与列修剪中，parquet结合spark可以实现分区过滤（spark sql，rdd的filter和where关键字），列修剪即获取所需要的列，列数越少查询的速率也就也快，parquet文件的metadata中存储了每个列的offset，可以只读相应列的数据

### 适用场景
1. 当读取的列数并非全部列数，建议使用parquet格式存储（建表时使用stored by parquet）；
2. 在进行列式计算或者向量计算时，建议也使用parquet格式存储，可以提高运算效率；
3. 如果有文件需要备份存储，可以使用parquet文件进行压缩，可以有效的节约空间，提高压缩效率和速率。
4. 嵌套数据类型

### 优化技巧
1. [impala控制parquet文件大小](https://www.cloudera.com/documentation/enterprise/latest/topics/impala_parquet_file_size.html)  
	set PARQUET\_FILE\_SIZE=512m
2. 按文件切片或者按照 `row group` 切片


## RCFile




## ORC




## Sequencefile







